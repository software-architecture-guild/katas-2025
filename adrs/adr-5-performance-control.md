# ADR 5: Measure Validation Time to Assess AI Effectiveness

**Date**: 2025-02-14

## Status

Proposed

## Context

To effectively evaluate AI’s impact on the grading and certification process, it is essential to measure the **time spent on validation tasks**. AI automation aims to enhance efficiency, but without accurate tracking, it is impossible to determine its true effectiveness. Establishing a time measurement system ensures that AI-driven improvements lead to measurable reductions in expert workload.

Currently, there is no structured mechanism to track validation time. Implementing this process before AI automation is introduced provides **baseline performance metrics**, allowing a direct comparison between manual and AI-assisted validation.

## Decision

To assess AI effectiveness in validation processes, the following measures will be implemented:

* **Track Expert Validation Time**: We assume that expert validation time tracking is already in place. If not, it should be implemented to measure how long experts spend grading each submission.
* **Establish Baseline Metrics**: Capture pre-automation validation times to compare against AI-assisted workflows.
* **Monitor AI Impact on Efficiency**: Regularly evaluate whether AI reduces validation time while maintaining grading quality.
* **Optimize Processes Based on Data**: Use time measurement insights to refine AI models and improve overall efficiency.

## Consequences

### Positive

* Enables objective assessment of AI’s contribution to efficiency improvements.
* Identifies areas where AI assistance can further optimize expert workload.
* Ensures AI-driven automation is effectively reducing validation time.
* Supports data-driven decision-making for AI deployment and enhancement.

### Negative

* Requires additional infrastructure for accurate tracking and analysis.
* Experts may initially resist time tracking if perceived as micromanagement.
* Data collection and analysis introduce operational overhead.

### Risks

* Inaccurate or incomplete time tracking could lead to misleading conclusions.
* Poorly defined baseline metrics may distort AI effectiveness assessments.
* Expert resistance may impact the completeness and reliability of collected data.

By measuring validation time, we establish a concrete framework for assessing AI effectiveness, ensuring automation leads to real efficiency gains without compromising grading quality.
